= Custom Metrics Autoscaling

In the previous section, we built a `HorizontalPodAutoscaler` resource that allows our workload to scale by adjusting the number of replicas in the `Deployment` resource. The HPA we created defines a set of threshold values for resource consumption metrics that the cluster automatically gathers from all workloads (i.e., memory and CPU consumption). The cluster can do this because the container runtime provides those metrics in a consistent fashion. Periodically, each HPA defined on the cluster is evaluated against the current metrics and the workloads they target are scaled if need be.

There are many cases, however, where scaling might better be driven by *other* metrics rather than simple resource consumption. As an example, consider the possibility of an event-driven architecture where the number of items in a queue drives the number of deployed `Pod` resources in the workload that processes that queue.

Another example would be workloads that would be able to scale to `0` when there was no demand for them. Because the HPA uses consumption metrics, scaling to zero isn't possible.

The OpenShift https://docs.openshift.com/container-platform/4.14/nodes/cma/nodes-cma-autoscaling-custom.html[Custom Metrics Autoscaler] provides the abstractions needed to drive scaling with arbitrary metrics, and this lab will demonstrate how to enable the CMA on your cluster and then configure it against a workload.

== CMA Architecture

The Custom Metrics Autoscaler works as a wrapper around the core HPA functionality rather than a parallel set of tools. In practice, this means that instead of defining a `HorizontalPodAutoscaler` resource, you'll define a `ScaledObject` that the CMA will use to manage an underyling HPA on your behalf. The `ScaledObject` also defines *where* the desired scaling information will come from (typyically a Prometheus datasource) in terms of an endpoint and a query. 

Most examples (including this one) will demonstrate the configuration and use of an in-cluster datasource (in this case, metrics scraped from the workload itself) 

== Lab Outline

- Configure user workload metrics on the cluster
- Install the CMA Operator
- Create a CMA instance
- Deploy a workload
- Create a `ScaledObject` resource
- Test the configuration

== User Workload Monitoring

While there is no requirement that the CMA use metrics that originate in-cluster, that is the most common use-case and is in fact how we'll do it in this lab. So the cluster first needs to be configured to scrape metrics from user workloads to make that possible, as the CMA needs to have metrics to query to drive it.

NOTE: Skip this section if you've already done the 'Day 2 | Observability' section of the lab, which includes configuring User Workload Monitoring. You can go directly to the 'Install the CMA' section.

. Check the cluster-monitoring-config ConfigMap object:
+
[source,sh,role=execute]
----
oc -n openshift-monitoring get configmap cluster-monitoring-config -o yaml
----
+
.Sample Output
[source,text,options=nowrap]
----
apiVersion: v1
data: {}
kind: ConfigMap
metadata:
  creationTimestamp: "2023-06-06T17:11:22Z"
  name: cluster-monitoring-config
  namespace: openshift-monitoring
  resourceVersion: "391968"
  uid: 5d84fef5-d798-4b11-bb2f-dd93fc6e76d8
----

. Enable User Workload Monitoring:
+
[source,sh,role=execute]
----
oc patch configmap cluster-monitoring-config -n openshift-monitoring \
  --patch='{"data":{"config.yaml": "enableUserWorkload: true\n"}}'
----

. Check that the User workload monitoring is starting up (wait until the output below matches what you see):
+
[source,sh,role=execute]
----
oc -n openshift-user-workload-monitoring get pods
----
+
.Sample Output
[source,text,options=nowrap]
----
NAME                                   READY   STATUS    RESTARTS   AGE
prometheus-operator-78774d88c8-vq2pz   2/2     Running   0          23m
prometheus-user-workload-0             6/6     Running   0          23m
prometheus-user-workload-1             6/6     Running   0          23m
thanos-ruler-user-workload-0           3/3     Running   0          23m
thanos-ruler-user-workload-1           3/3     Running   0          23m
----

=== Install the CMA

TIP: For the purposes of the lab, we're going to install the Custom Metrics Autoscaler Operator via the cluter's web console and the OperatorHub. On a production cluster it's generally recommended to use the GitOps pattern for cluster-level configuration like this. 

. In the OpenShift Container Platform web console, click *Operators → OperatorHub.*
. Type 'Custom Metrics' into the filter box and select the Red Hat version of the Operator.
+
image::../media/cma-operatorhub.png[Operator Hub]
. Click *Install*.
. On the *Install Operator* page for the CMA, accept the default settings.
. Click *Install*.
. Wait until the Operator has installed.
+
image::../media/cma-operatorhub-install.png[CMA Install]

. After all you have installed the CMA Operator, click *Operators → Installed Operators* to verify that it was installed.
+
image::../media/cma-show-installed-operators.png[Installed Operators]

=== Create a KEDA Controller

The installation of the Operator allows us to deploy an actual Controller that performs the adapter functions discussed at the beginning of the lab. Again, we'll use the web console to make creating this easy:

. We'll create a Resource of type `KedaController` by clicking on the *Custom Metrics Autoscaler* in the *Installed Operators* view.

. Make sure to select the Project of `openshift-keda` in the *Project* drop-down at the top of the page.

+
image::../media/cma-keda-controller-create.png[Installed Operators]

. Click 'Create instance' in the `KedaController` panel.

+
image::../media/cma-keda-controller-configure.png[Installed Operators]

. Verify that the instance name defaults to `keda` and then click the *Create* button at the bottom of the page.

. After a few seconds, you can verify that it installed correctly by looking at the running Pods in the `openshift-keda` namespace:
+
[source,sh,role=execute]
----
oc -n openshift-keda get pods
----
+
.Sample Output
[source,text,options=nowrap]
----
NAME                                                  READY   STATUS    RESTARTS   AGE
custom-metrics-autoscaler-operator-6cbc75447c-gtldf   1/1     Running   0          14m
keda-admission-75fb5fcbcf-znj2d                       1/1     Running   0          118s
keda-metrics-apiserver-65b86548c4-2mqzq               1/1     Running   0          118s
keda-operator-7944475998-cqp55                        1/1     Running   0          119s
----

=== Demo Workload

The CMA needs custom metrics to query, so we need to create a demo workload next.

We're going to create one that literally just responds to HTTP requests and increments a request counter.

. Start by creating a new Project, either via the web console or with the `oc new-project cma-demo` command. In the web console, you'll probably want to switch to the 'Developer' view available at the top of menu.
For the purposes of this demo and making the code snippets work, use `cma-demo` as the project name.

. The default view of a new Project in the Developer view is the 'Add' screen:
+
image::../media/cma-demo-workload-create.png[Workload Creation]
Click the 'Import YAML' button in the 'From Local Machine' panel because we're going to pull a pre-built container image from an external registry.

. Cut and paste in this YAML:

.Demo Deployment
[source,yaml,role=execute]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: cma-demo-app
    name: cma-demo-app
spec:
  replicas: 1
  selector:
    matchLabels:
      app: cma-demo-app
  template:
    metadata:
      labels:
        app: cma-demo-app
        type: keda-testing
    spec:
      containers:
      - name: prom-demo-app
        image: quay.io/zroubalik/prometheus-app:latest
        imagePullPolicy: IfNotPresent
        securityContext:
          allowPrivilegeEscalation: false
          runAsNonRoot: true
          capabilities:
            drop:
              - ALL
          seccompProfile:
            type: RuntimeDefault
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: cma-demo-app
  annotations:
    prometheus.io/scrape: "true"
  name: cma-demo-app
spec:
  ports:
  - name: http
    port: 80
    protocol: TCP
    targetPort: 8080
  selector:
    type: keda-testing
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  labels:
  name: keda-testing-sm
spec:
  endpoints:
  - scheme: http
    port: http
  namespaceSelector: {}
  selector:
    matchLabels:
       app: cma-demo-app
----

This will create the `Deployment` and `Service` resources, as well as the necessary `ServiceMonitor` resource that instructs the User Workload Monitoring stack to gather metrics from it.

. Make sure the workload has started
+
[source,sh,role=execute]
----
oc -n cma-demo get pods
----
+
.Sample Output
[source,text,options=nowrap]
----
NAME                            READY   STATUS    RESTARTS   AGE
cma-demo-app-7cbc9968bb-v9zbz   1/1     Running   0          1m
----

=== CMA Authentication

The next step is to provide the CMA an identity to allow it to query the API for this custom metrics data our app is producing. The CMA allows for a unique identity per scaled resource, so there is a lot of flexibility and granularity in terms of controlling access to this data.

. Create a `ServiceAccount`
+
[source,sh,role=execute]
----
oc -n cma-demo create serviceaccount thanos
----
+
[source,sh,role=execute]
----
oc -n cma-demo describe sa thanos
----
+
.Sample Output
[source,text,options=nowrap]
----
Name:                thanos
Namespace:           cma-demo
Labels:              <none>
Annotations:         <none>
Image pull secrets:  thanos-dockercfg-x5dmh
Mountable secrets:   thanos-dockercfg-x5dmh
Tokens:              thanos-token-nng46
Events:              <none>
----
+
You'll need the value in the `Tokens` field for the next step.
. Create an `AuthenticationTrigger` for the CMA. You'll need to replace the `name` key in the two parameters with the value from the `Tokens` field in the previous step.
+
[source,sh,role=execute]
----
cat <<EOF | oc -n cma-demo apply -f -
apiVersion: keda.sh/v1alpha1
kind: TriggerAuthentication
metadata:
  name: keda-trigger-auth-prometheus
spec:
  secretTargetRef:
  - parameter: bearerToken
    name: thanos-token-nng46
    key: token
  - parameter: ca
    name: thanos-token-nng46
    key: ca.crt
EOF
----

=== Create a `Role` for the `ServiceAccount`

We have the `ServiceAccount`, now we need a `Role` that gives it the correct permissions on the OCP API to retrieve data.

. Create the `Role`:
+
[source,sh,role=execute]
----
cat <<EOF | oc -n cma-demo apply -f -
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: thanos-metrics-reader
rules:
- apiGroups:
  - ""
  resources:
  - pods
  verbs:
  - get
- apiGroups:
  - metrics.k8s.io
  resources:
  - pods
  - nodes
  verbs:
  - get
  - list
  - watch
EOF
----
+
. Bind it to the `ServiceAccount`
+
[source,sh,role=execute]
----
oc adm policy add-role-to-user thanos-metrics-reader -z thanos --role-namespace=cma-demo
----

=== Create the `ScaledObject`

Finally, it's time to tell the CMA how to scale the workload based on a custom metric:

. Create the `ScaledObject` resource:
+
[source,sh,role=execute]
----
cat <<EOF | oc -n cma-demo apply -f -
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: prometheus-scaledobject
spec:
  scaleTargetRef:
    name: cma-demo-app
  minReplicaCount: 1
  maxReplicaCount: 10
  triggers:
  - type: prometheus
    metadata:
      serverAddress: https://thanos-querier.openshift-monitoring.svc.cluster.local:9092
      namespace: cma-demo
      authModes: "bearer"
      metricName: http_requests_total
      threshold: '5'
      query: sum(rate(http_requests_total{job="cma-demo-app"}[2m]))
    authenticationRef:
      name: keda-trigger-auth-prometheus
EOF
----
+
Note the reference to the `Deployment` we created earlier, both in the `scaleTargetRef` (which identifies the workload to be scaled) as well as in the actual `query` field.
+
Also note the reference to the `TriggerAuthentication` resource in the `authenticationRef` field.
+
Finally, note that just like the HPA, CMA allows for hard `min` and `max` values for the scaled workload.
+
The `threshold` field sets the scaling factor on a *per-replica* basis, so if there is one `Pod` but `11` requests per 2 minutes, it will scale to `3` replicas.
+
. Validate the `ScaledObject`
+
[source,sh,role=execute]
----
oc -n cma-demo get so prometheus-scaledobject
----
+
.Sample Output
[source,text,options=nowrap]
----
NAME                      SCALETARGETKIND      SCALETARGETNAME   MIN   MAX   TRIGGERS     AUTHENTICATION                 READY   ACTIVE   FALLBACK   PAUSED    AGE
prometheus-scaledobject   apps/v1.Deployment   cma-demo-app      1     10    prometheus   keda-trigger-auth-prometheus   True    False    False      Unknown   2m40s
----
+ 
We want to make sure the `READY` column is `True`

=== Test the `ScaledObject` resource

Finally, we'll use the `Batch` API to run a bunch of Pods to generate some load and demonstrate the CMA doing its job:

. Create the `Job`:
+
[source,sh,role=execute]
----
cat <<EOF | oc -n cma-demo create -f -
apiVersion: batch/v1
kind: Job
metadata:
  generateName: generate-requests-
spec:
  template:
    spec:
      containers:
      - image: quay.io/zroubalik/hey
        name: test
        command: ["/bin/sh"]
        args: ["-c", "for i in $(seq 1 30);do echo $i;/hey -c 5 -n 100 http://cam-demo-app.cma-demo.svc;sleep 1;done"]
        securityContext:
          allowPrivilegeEscalation: false
          runAsNonRoot: true
          capabilities:
            drop:
              - ALL
          seccompProfile:
            type: RuntimeDefault
      restartPolicy: Never
  activeDeadlineSeconds: 120
  backoffLimit: 2
EOF
----
+
. Watch the workload to see it scale:
+
[source,sh,role=execute]
----
watch -n cma-demo oc get deployment cma-demo-app
----
+
.Sample Output
[source,text,options=nowrap]
----
NAME                      SCALETARGETKIND      SCALETARGETNAME   MIN   MAX   TRIGGERS     AUTHENTICATION                 READY   ACTIVE   FALLBACK   PAUSED    AGE
prometheus-scaledobject   apps/v1.Deployment   cma-demo-app      1     10    prometheus   keda-trigger-auth-prometheus   True    False    False      Unknown   2m40s
----


